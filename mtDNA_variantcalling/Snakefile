import os
import re
import json

# Config and Reference
# To only include mtDNA, 
REFERENCE = "/export/elixiruibfs/Elixir_Dolan/Kveik_Yeast_2022/reference-genome/NC_001224.fna"
FASTQ_DIR = "/export/elixiruibfs/Elixir_Dolan/Kveik_Yeast_2023/Fastqs"


onsuccess:
    print("Workflow finished, no error")
    shell("send_message.py 'mtPhylo pipeline finished, no error'")

onerror:
    print("An error occurred, see the log")
    shell("send_message.py 'An error occured in the mtPhylo pipeline, see the log: {log}'")
    
onstart:
    print ("----------------------------------------------------------------------------")
    print ("Starting mtPhylo analysis pipeline")
    print ("----------------------------------------------------------------------------")
    shell('send_message.py "mtPhylo pipeline started at $(date)"')




# Function to get sample names by trimming suffixes from filenames
## automatically retrieves all the samples

def get_samples():
    fastqdir = FASTQ_DIR
    suffix1 = "_R1_001.fastq.gz"
    suffix2 = "_R1_merged_001.fastq.gz"
    suffix3 = "_1.fastq"
    return [f.replace(suffix1, "").replace(suffix2, '').replace(suffix3, '') 
	for f in os.listdir(fastqdir) if f.endswith(suffix1) or f.endswith(suffix2) or f.endswith(suffix3)]
    
SAMPLES = get_samples()
#print (SAMPLES)

print("Found "+ str(len(SAMPLES)) +" samples.")

# Function to get the R1 and R2 fastq files for paired-end samples
def get_illumina_fastqs(wildcards):
    fastq_dir = FASTQ_DIR
    r1 = [os.path.join(fastq_dir, f) for f in os.listdir(fastq_dir) 
		if (re.search('^' + wildcards.sample + ".*R1_.*001\\.fastq\\.gz$", f) or re.search('^' + wildcards.sample + ".*_1\\.fastq$", f) )]
    r2 = [os.path.join(fastq_dir, f) for f in os.listdir(fastq_dir)
                if (re.search('^' + wildcards.sample + ".*R2_.*001\\.fastq\\.gz$", f) or re.search('^' + wildcards.sample + ".*_2\\.fastq$", f) )]
    #print("Sample: " + wildcards.sample)
    #print('  - R1: ' + r1[0] + ' R2:' + r2[0] )

    return ([r1[0], r2[0]])
#

# Getting the samples
SAMPLES = get_samples()
print('Found the following samples')
print(SAMPLES)

# Rule: Define the final output of the pipeline
rule all:
    input:
        expand("fastp/{sample}_R1.trimmed.fastq.gz", sample=SAMPLES),
        expand("fastp/{sample}_R2.trimmed.fastq.gz", sample=SAMPLES),
        expand("fastp/{sample}.fastp_report.html", sample=SAMPLES),
        expand("fastp/{sample}.fastp_report.json", sample=SAMPLES),
        expand("aligned/{sample}.aligned.bam", sample=SAMPLES),
        expand("aligned/{sample}.aligned.bam.bai", sample=SAMPLES),
        expand("aligned/{sample}.filtered.bam", sample=SAMPLES),
        expand("aligned/{sample}.filtered.bam.bai", sample=SAMPLES),
        expand("aligned/{sample}.coverage.txt", sample=SAMPLES),
        expand("aligned/{sample}.genome_coverage.txt", sample=SAMPLES),
        "multiqc_report/multiqc_report.html",
        expand("vcf/{sample}.vcf", sample=SAMPLES),
        expand("filtered_vcf/{sample}.filtered.vcf", sample=SAMPLES),
        expand("filtered_vcf/{sample}.filtered.vcf.gz.tbi", sample=SAMPLES),
        expand("snps/{sample}.snp_summary.txt", sample=SAMPLES),
        expand("consensus/{sample}.consensus.fa", sample=SAMPLES),
        expand("consensus/{sample}.final_consensus.fa", sample=SAMPLES),
        "iqtree/Samples-GTR.contree", "iqtree/Samples-GTRI.contree", "iqtree/Samples-MFP.contree", 


# Rule: Setup (create directories)
rule setup:
    output: ".setup_done"
    shell:
        "mkdir -p fastp aligned fastq multiqc_report vcf filtered_vcf snps consensus iqtree  && touch .setup_done"

# Rule: Adapter removal and quality trimming with fastp
rule fastp_pe:
    conda: "envs/fastp.yaml"
    input:
        lambda wildcards: get_illumina_fastqs(wildcards)
    output:
        r1="fastp/{sample}_R1.trimmed.fastq.gz",
        r2="fastp/{sample}_R2.trimmed.fastq.gz",
        html="fastp/{sample}.fastp_report.html",
        json="fastp/{sample}.fastp_report.json"
    threads: 16
    shell:
        """
        fastp -w {threads} -i {input[0]} -I {input[1]} -o {output.r1} -O {output.r2} \
           --qualified_quality_phred 20 --length_required 30 --low_complexity_filter -j {output.json} -h {output.html}
        """
# Index the reference:
rule bwa_index:
    input:
        REFERENCE,
    output:
        idx=multiext(REFERENCE, ".amb", ".ann", ".bwt", ".pac", ".sa"),
    log:
        "logs/bwa_index.log"
    
    wrapper:
        "v5.5.2/bio/bwa/index"



# Rule: BWA mapping (paired-end reads)
rule bwa_aln:
    conda: "envs/bwaaln.yaml"
    input:
        r1="fastp/{sample}_R1.trimmed.fastq.gz",
        r2="fastp/{sample}_R2.trimmed.fastq.gz",
        ref=REFERENCE,
        idx=multiext(REFERENCE, ".amb", ".ann", ".bwt", ".pac", ".sa")
    output:
        bam="aligned/{sample}.aligned.bam",
        bai="aligned/{sample}.aligned.bam.bai"
    log: "logs/{sample}.bwa_aln.log"    
    threads: 8
    shell:
        """
        bwa aln -t {threads} {input.ref} {input.r1} > {wildcards.sample}_R1.sai 2> {log}
        bwa aln -t {threads} {input.ref} {input.r2} > {wildcards.sample}_R2.sai 2>> {log}

        RGSTR="@RG\\tID:{wildcards.sample}\\tSM:{wildcards.sample}"

        bwa sampe -r "$RGSTR" {input.ref} {wildcards.sample}_R1.sai {wildcards.sample}_R2.sai {input.r1} {input.r2} | \
        samtools sort -@ {threads} -o {output.bam} 2>&1 >>{log}

        samtools index {output.bam} 2>&1 >>{log}

        rm {wildcards.sample}_R1.sai {wildcards.sample}_R2.sai
        """

# Rule: Filter BAM by mapping quality (using samtools)
rule samtools_filter:
    conda: "envs/samtools.yaml"
    input:
        bam="aligned/{sample}.aligned.bam",
        bai="aligned/{sample}.aligned.bam.bai"
    output:
        bam="aligned/{sample}.filtered.bam",
        bai="aligned/{sample}.filtered.bam.bai"
    threads: 16
    shell:
        """
        samtools view -@ {threads} -bq 30 {input.bam} | samtools sort -@ {threads} -o {output.bam}
        samtools index {output.bam}
        """

# Rule: Calculate coverage using samtools depth
rule calculate_coverage:
    conda: "envs/samtools.yaml"
    input:
        bam="aligned/{sample}.filtered.bam",
        bai="aligned/{sample}.filtered.bam.bai"
    output:
        coverage="aligned/{sample}.coverage.txt"
    shell:
        """
        samtools depth -aa {input.bam} > {output.coverage}
        """

# Rule: Calculate coverage using bedtools genomecov
rule calculate_genome_coverage:
    conda: "envs/bedtools.yaml"
    input:
        bam="aligned/{sample}.filtered.bam",
        bai="aligned/{sample}.filtered.bam.bai",
    output:
        coverage="aligned/{sample}.genome_coverage.txt"
    shell:
        """
        bedtools genomecov -ibam {input.bam} > {output.coverage}
        """

# Rule: Generate MultiQC report
rule multiqc:
    conda: "envs/multiqc.yaml"
    input:
        expand("fastp/{sample}.fastp_report.json", sample=SAMPLES),
        expand("aligned/{sample}.aligned.bam", sample=SAMPLES),
        expand("aligned/{sample}.filtered.bam", sample=SAMPLES),
        expand("aligned/{sample}.coverage.txt", sample=SAMPLES),
        expand("aligned/{sample}.genome_coverage.txt", sample=SAMPLES)
    output:
        "multiqc_report/multiqc_report.html"
    shell:
        """
        multiqc -o multiqc_report .
        """

# Rule: Generate VCF files using bcftools
rule bcftools_call:
    conda: "envs/bcftools.yaml"
    input:
        bam="aligned/{sample}.filtered.bam",
        bai="aligned/{sample}.filtered.bam.bai",
        ref=REFERENCE
    output:
        vcf="vcf/{sample}.vcf"
    threads: 8
    shell:
        """
        bcftools mpileup -f {input.ref} -q 30 {input.bam} | \
        bcftools call -mv --ploidy 1 -o {output.vcf}
        """

# Rule: Filter VCF files using vcftools
rule vcftools_filter:
    conda: "envs/vcftools.yaml"
    input:
        vcf="vcf/{sample}.vcf"
    output:
        filtered_vcf="filtered_vcf/{sample}.filtered.vcf"
    threads: 4
    shell:
        """
        vcftools --vcf {input.vcf} --remove-indels --recode --minDP 3 --out filtered_vcf/{wildcards.sample}.filtered
        mv filtered_vcf/{wildcards.sample}.filtered.recode.vcf {output.filtered_vcf}
        """

# Rule: Compress and index filtered VCF files using bgzip and tabix
rule bgzip_tabix:
    conda: "envs/samtools.yaml"
    input:
        filtered_vcf="filtered_vcf/{sample}.filtered.vcf"
    output:
        filtered_vcf_gz="filtered_vcf/{sample}.filtered.vcf.gz",
        tbi="filtered_vcf/{sample}.filtered.vcf.gz.tbi"
    shell:
        """
        bgzip -c {input.filtered_vcf} > {output.filtered_vcf_gz}
        tabix -p vcf {output.filtered_vcf_gz}
        """

rule snp_summary:
    conda: "envs/bcftools.yaml"
    input:
        filtered_vcf_gz="filtered_vcf/{sample}.filtered.vcf.gz",
        tbi="filtered_vcf/{sample}.filtered.vcf.gz.tbi"
    output:
        summary="snps/{sample}.snp_summary.txt"
    shell:
        """
        # Check if the VCF file is empty or malformed
        if [ $(bcftools view -H {input.filtered_vcf_gz} | wc -l) -eq 0 ]; then
            echo "No SNPs found or the VCF file is empty for {wildcards.sample}" > {output.summary}
        else
            # Count each SNP type and write to summary file
            bcftools query -f '%REF\t%ALT\n' {input.filtered_vcf_gz} | awk '{{print $1 ">" $2}}' | sort | uniq -c | awk '{{print $2, $1}}' > {output.summary}

            # Calculate and append the total number of SNPs
            TOTAL_SNPS=$(bcftools view -H -v snps {input.filtered_vcf_gz} | wc -l)
            echo -e "Total SNPs\t$TOTAL_SNPS" >> {output.summary}
        fi
        """

# Rule: Generate consensus FASTA files from VCF using bcftools
rule consensus_fasta:
    conda: "envs/bcftools.yaml"
    input:
        vcf="filtered_vcf/{sample}.filtered.vcf.gz",
        ref=REFERENCE
    output:
        consensus_fa="consensus/{sample}.consensus.fa"
    threads: 4
    shell:
        """
        cat {input.ref} | bcftools consensus {input.vcf} > {output.consensus_fa}
        """


# Rule: Modify FASTA headers with sample name
rule modify_fasta_headers:
    input:
        consensus_fa="consensus/{sample}.consensus.fa"
    output:
        final_consensus_fa="consensus/{sample}.final_consensus.fa"
    shell:
        """
        SAMPLE_NAME=$(basename {input.consensus_fa} .consensus.fa)
        # Remove the trailing dot from SAMPLE_NAME, if any
        SAMPLE_NAME=${{SAMPLE_NAME%.}}
        # Replace the current header with >SAMPLE_NAME
        sed '1s/.*/>'"$SAMPLE_NAME"'/' {input.consensus_fa} > {output.final_consensus_fa}
        """

# Rule: Concatenate all final consensus FASTA files into a single file
rule concatenate_fasta:
    conda: "envs/samtools.yaml"
    input:
        expand("consensus/{sample}.final_consensus.fa", sample=SAMPLES)  # Ensure all final consensus files are included
    output:
        "Samples.fa"
    shell:
        """
        cat {input} > {output}
        """


# Rule: Perform phylogenetic tree inference with IQ-TREE
rule iqtree_GTR:
    conda: "envs/iqtree.yaml"
    input:
        "Samples.fa"
    output:
        "iqtree/Samples-GTR.contree"  
    log: "logs/iqtree-GTR.log"
    shell:
        """
        mkdir -p $(dirname {output})  # Ensure the iqtree directory exists
        iqtree2 -s {input} -m GTR -bb 1000 -nt AUTO -pre iqtree/Samples-GTR > {log} 2>&1
        """

# Rule: Perform phylogenetic tree inference with IQ-TREE
rule iqtree_GTRI:
    conda: "envs/iqtree.yaml"
    input:
        "Samples.fa"
    output:
        "iqtree/Samples-GTRI.contree"  
    log: "logs/iqtree-GTRI.log"
    shell:
        """
        mkdir -p $(dirname {output})  # Ensure the iqtree directory exists
        iqtree2 -s {input} -m GTR+I -bb 1000 -nt AUTO -pre iqtree/Samples-GTRI > {log} 2>&1
        """

rule iqtree_MFP:
    conda: "envs/iqtree.yaml"
    input:
        "Samples.fa"
    output:
        "iqtree/Samples-MFP.contree"  
    log: "logs/iqtree-MFP.log"
    shell:
        """
        mkdir -p $(dirname {output})  # Ensure the iqtree directory exists
        iqtree2 -s {input} -m MFP -bb 1000 -nt AUTO -pre iqtree/Samples-MFP > {log} 2>&1
        """